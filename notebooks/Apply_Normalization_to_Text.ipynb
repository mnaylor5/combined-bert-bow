{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Apply Normalization to Text",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnaylor5/combined-bert-bow/blob/main/notebooks/Apply_Normalization_to_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F38PxgdqFJOF"
      },
      "source": [
        "# Prep Data for Modeling\n",
        "This started out as a copy of the modeling notebook, but I'm repurposing it for the normalization code. \n",
        "\n",
        "Install Git LFS and clone our repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8YjS5TejSiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ba1525-0ce6-4b05-87a6-3a50afc72f74"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs && git lfs install\n",
        "!git clone https://github.com/mnaylor5/combined-bert-bow.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 104 not upgraded.\n",
            "Need to get 6,229 kB of archives.\n",
            "After this operation, 14.5 MB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 2.13.3 [6,229 kB]\n",
            "Fetched 6,229 kB in 1s (10.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 160841 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.13.3_amd64.deb ...\n",
            "Unpacking git-lfs (2.13.3) ...\n",
            "Setting up git-lfs (2.13.3) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS initialized.\n",
            "Cloning into 'combined-bert-bow'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 31 (delta 9), reused 12 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDyJR1DyiOvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a61bc53-c04f-4249-83bb-630eb170d816"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6vrhoB18IMV"
      },
      "source": [
        "# Pre-processing of raw data\n",
        "Starting with the dataframe Vedant put in Git (first PR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99uCftDRiOwB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "60da9e25-c15b-4ab4-f7e0-1c566c759e43"
      },
      "source": [
        "text = pd.read_csv(r'combined-bert-bow/data/CaptumAttributionDF.csv')\n",
        "MAX_LEN = 512\n",
        "texts = pd.DataFrame(text)\n",
        "texts.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>My dad is hospitalized and had to be put in a ...</td>\n",
              "      <td>['', '', '', '', '', '', '', '', '', '', 'h', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Feels like anxiety is turning my brain to stew...</td>\n",
              "      <td>['F', 'e', 'e', 'l', '', '', 'l', '', 'k', 'e'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>What do you do when anxiety kicks in around ot...</td>\n",
              "      <td>['W', 'h', '', '', '', '', '', '', '', '', 'u'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Finding a job is a nightmare I want to rant ab...</td>\n",
              "      <td>['F', '', 'n', '', '', 'n', 'g', '', '', '', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>All I want is to be able to sleep again... I g...</td>\n",
              "      <td>['', 'l', 'l', '', '', '', 'w', '', 'n', '', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                          text_norm\n",
              "0      1  ...  ['', '', '', '', '', '', '', '', '', '', 'h', ...\n",
              "1      1  ...  ['F', 'e', 'e', 'l', '', '', 'l', '', 'k', 'e'...\n",
              "2      1  ...  ['W', 'h', '', '', '', '', '', '', '', '', 'u'...\n",
              "3      1  ...  ['F', '', 'n', '', '', 'n', 'g', '', '', '', '...\n",
              "4      1  ...  ['', 'l', 'l', '', '', '', 'w', '', 'n', '', '...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCreZvUMwm8"
      },
      "source": [
        "## Create a Bag of Words\n",
        "The three steps to do so are:\n",
        "\n",
        "\n",
        "1.   Remove punctuation\n",
        "2.   Lemmatize the text\n",
        "3.   Remove stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-arDLWrocCj"
      },
      "source": [
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "STOPS = stopwords.words('english') # already lowercased and split contractions\n",
        "def normalize_text(x, stop_words=STOPS, min_word_len=2):\n",
        "  # lowercase\n",
        "  x = x.lower()\n",
        "\n",
        "  # remove punctuation (everything but letters or spaces)\n",
        "  x = re.sub(r'[^a-z\\s]', ' ', x)\n",
        "\n",
        "  # tokenize by spacing and remove stop words / super short words\n",
        "  tokens = re.split(r'\\s+', x)\n",
        "  tokens = filter(lambda x: (x not in stop_words) & (len(x) > min_word_len),\n",
        "                  tokens) \n",
        "  \n",
        "  # lemmatize now\n",
        "  tokens = map(lemmatizer.lemmatize, tokens)\n",
        "  \n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGBFgh8TpGw-",
        "outputId": "711b4e5a-05de-4177-b9dd-71652d3c7582"
      },
      "source": [
        "text['text'].head(10).apply(normalize_text).values"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['dad hospitalized put breathing machine help breathe trying keep together family life asia usa found last night dad hospitalized due hard time breathe escalated breathing tube mom freaking trying positive anxiety high low fly asia mid week giving anxiety added patiently waiting work grant leave know monday part want fly think could handle seeing hooked ventilator mom putting weight shoulder taking care everything saturday agony waiting till monday take care necessary paperwork get approved causing much anxiety trying positive everything thought hooked ventilator causing shut hoping progress recovery get',\n",
              "       'feel like anxiety turning brain stew lately fucking stew emotion anxiety consider emotional person jesus christ brain feel like jumble emotion anxious rumination fog make sense every night basically turning worry session sun rise lay thinking shitty feel everything life could wrong shit thinking people care probably hate reason hardly explain remember acting like dumbass mainly due guessed anxiety gaining bit clarity lot easier explain reason remember telling ton unfunny joke point sheer embarrassment mainly feared funny social life going decline kept forcing try funny haze brainfog anxiety feeling unreality general feeling fucked fucked fucked even though realize blowing proportion even know man mean think little clearly actually reason feel like fucked jesus know trick get anxiety fog ridden worry confused scrambled rumination',\n",
              "       'anxiety kick around people wave anxiety hit company currently laying bed paralyzed fear silently sobbing anxiety getting worse minute anyone method calming even thing seem getting worse',\n",
              "       'finding job nightmare want rant job hunting many thing trigger anxiety looking job writing sending application everything right hate wrote something stupid qualified enough make typo mistake checking email phone reply application receive yet another rejection job really wanted call interviewer schedule interview social anxiety really really hate phone call actual interview spending hour judged social behaviour everything say several people expert judging others unknown place complete stranger get much worse someone social anxiety ruin almost every interview super nervours force fake confidence pretend something confident sociable person good course people want someone good working together team function lot stress opposite',\n",
              "       'want able sleep bed night wondering get sleep terrified nightmare may may come nightmare wake screaming shivering heart pounding mouth cope day around people trust whole different ball game alone need right rest sure one need rest deserve',\n",
              "       'date girl really like thursday need advice anxiety attack front pretty bad health anxiety linked heart taking med high blood pressure currently experiencing symptom unstable leg like floor waving standing tense upset stomach feeling breathing enough air constant fear fainting losing control since anxiety started january pretty much alone room briefly leaving house walk meeting wonderful opportunity ahead girl short lasted girlfriend mine early never chance get closer stuff happened yesterday texted nowhere asking etc long story short back home town invited drink thursday afternoon always spark extremely happy new thing evolved romance come current barrier way terrible anxiety feeling simply want enjoy time making smile happy know anxiety probably ruin everything imagine feeling instability feel horrible thing throat increased heart rate thinking normal crazy something like approach situation overcome fear anxiety attack front set record straight right away tell anxiety grade student probably would understand cancel everything stay safe home would shoe',\n",
              "       'think ptsd past relationship last romantic relationship bad good amount emotional mental abuse always walking life playing emotion every time sense even slightest hiccup type relationship immediately think worst get anxious panicky assume gonna walk million thing run mind one else issue maybe ptsd right word think else explain',\n",
              "       'anyone develop ulcer stress anxiety brings stress level high since highschool year ago finally reached point got diagnosed ulcer',\n",
              "       'stop ocd year old ocd gotten much worse year worry stress literally everything keep thinking thing never even constantly back check see gotten point cant even use phone without thinking messaged someone randomly called someone use customer service send simple message start think something like insulted person said something didnt mean even though reality try think actually seems like brain make scenario even actually didnt someone family get sick start worry going die need hospital feel like going crazy going end ruining life get control worrying stop possible without using medicine',\n",
              "       'help pls got drunk call older sister sad home moment got real bad anxiety sleep'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg2idmp0rxsF",
        "outputId": "7d7e73f2-133e-4a3e-90f2-eb5183c80ece"
      },
      "source": [
        "%%time\n",
        "text['text_norm'] = text['text'].apply(normalize_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 32s, sys: 659 ms, total: 2min 33s\n",
            "Wall time: 2min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJszfqHvtgV"
      },
      "source": [
        "Save the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG2Owxn8vuxg"
      },
      "source": [
        "text.to_csv('normalized_depression_text.csv', index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkpFpTiXIKXt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}